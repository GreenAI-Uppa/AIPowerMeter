
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Deep learning benchmark &#8212; AI Power Meter  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Machine and Deep Learning Benchmarks with wattmeters" href="schneiderbox.html" />
    <link rel="prev" title="Dev Documentation" href="../usage/modules.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="deep-learning-benchmark">
<h1>Deep learning benchmark<a class="headerlink" href="#deep-learning-benchmark" title="Permalink to this heading">¶</a></h1>
<div class="section" id="experimental-protocol">
<h2>Experimental protocol<a class="headerlink" href="#experimental-protocol" title="Permalink to this heading">¶</a></h2>
<p>We’ve started to run experiments to measure the energy consumption of classical deep learning pretrained model at inference. Our protocol acts as follows:</p>
<ul class="simple">
<li><p>we load a pretrained architecture,</p></li>
<li><p>we select an input size (resolution for Computer Vision, number of tokens for NLP),</p></li>
<li><p>we run x inferences and measure power draws with AIPowerMeter,</p></li>
<li><p>we repeat the experiment 10 times to have more robustness.</p></li>
</ul>
<p>For each set of experiments, power measurements and lattencies are written into several power_metrics.json  and latency.csv files: one by tuple (format,x) where format is the chosen input size an integer and x =0, … 9 following this architecture:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>input_format
├── run_x
    ├── power_metrics.json
    └── latency.csv
</pre></div>
</div>
<p>We then compile  <a class="reference external" href="https://github.com/GreenAI-Uppa/AIPowerMeter/blob/main/deep_learning_benchmark/concat_power_measure.py">here</a> an estimate of different power draws of one inference and compile the median of the over the 10 runs. For each pretrained model, results are generated into a csv file where each row corresponds to one input size and each column represents the median of one power draw.</p>
</div>
<div class="section" id="alexnet-study">
<h2>Alexnet study<a class="headerlink" href="#alexnet-study" title="Permalink to this heading">¶</a></h2>
<p>As a gentle start, we measure the consumption at inference of a vanilla <a class="reference external" href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">Alexnet</a> on a GeForce RTX 3090 GPU, and 16 i9 Intel cores CPU.</p>
<p>We first import necessary modules for power draws and torch models downloads.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span><span class="o">,</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">deep_learning_power_measure.power_measure</span> <span class="kn">import</span> <span class="n">experiment</span><span class="p">,</span> <span class="n">parsers</span>
<span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="nn">models</span>
<span class="kn">import</span> <span class="nn">torch</span>
</pre></div>
</div>
<p>We then load Alexnet model and push it into our GeForce RTX 3090 GPU.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#load your favorite model</span>
<span class="n">alexnet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">alexnet</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">#choose your favorite device</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Using </span><span class="si">{}</span><span class="s1"> device&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>

<span class="c1">#load the model to the device</span>
<span class="n">alexnet</span> <span class="o">=</span> <span class="n">alexnet</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>We then prepare the experiment protocol described above for a list of 20 input sizes from 112*112 to 2016*2016 resolution sizes. We choose to run an adequate number of inferences for each input size to let AIPowerMeter reports the power draws during around 40 seconds.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#experiments protocol</span>

<span class="c1">#resolution size list</span>
<span class="n">input_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">112</span><span class="o">*</span><span class="p">(</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">)]</span>
<span class="c1">#number of experiments</span>
<span class="n">xps</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
<p>We then start the inferences and measurements for each input size and each experiment.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#start of the experiments</span>
<span class="k">for</span> <span class="n">u</span><span class="p">,</span><span class="n">input_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_sizes</span><span class="p">):</span>
  <span class="c1">#number of inferences</span>
  <span class="n">iters</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mi">40000</span><span class="o">/</span><span class="p">(</span><span class="n">u</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
  <span class="c1">#create a random image</span>
  <span class="n">image_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">input_size</span><span class="p">,</span><span class="n">input_size</span><span class="p">)</span>
  <span class="n">image_test</span> <span class="o">=</span> <span class="n">image_test</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="c1">#start of the experiments</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">xps</span><span class="p">):</span>
      <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Experience&#39;</span><span class="p">,</span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;/&#39;</span><span class="p">,</span><span class="n">xps</span><span class="p">,</span><span class="s1">&#39;is running&#39;</span><span class="p">)</span>
      <span class="n">latencies</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="c1">#AIPM</span>
      <span class="n">input_image_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">input_size</span><span class="p">,</span><span class="n">input_size</span><span class="p">)</span>
      <span class="n">driver</span> <span class="o">=</span> <span class="n">parsers</span><span class="o">.</span><span class="n">JsonParser</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span><span class="s2">&quot;input_&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;/run_&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">k</span><span class="p">)))</span>
      <span class="n">exp</span> <span class="o">=</span> <span class="n">experiment</span><span class="o">.</span><span class="n">Experiment</span><span class="p">(</span><span class="n">driver</span><span class="p">)</span>
      <span class="n">p</span><span class="p">,</span> <span class="n">q</span> <span class="o">=</span> <span class="n">exp</span><span class="o">.</span><span class="n">measure_yourself</span><span class="p">(</span><span class="n">period</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
      <span class="n">start_xp</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
      <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
          <span class="c1">#print(t)</span>
          <span class="n">start_iter</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
          <span class="n">y</span> <span class="o">=</span> <span class="n">alexnet</span><span class="p">(</span><span class="n">image_test</span><span class="p">)</span>
          <span class="n">res</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">start_iter</span>
          <span class="c1">#print(t,&#39;latency&#39;,res)</span>
          <span class="n">latencies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
      <span class="n">q</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">experiment</span><span class="o">.</span><span class="n">STOP_MESSAGE</span><span class="p">)</span>
      <span class="n">end_xp</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;power measuring stopped after&quot;</span><span class="p">,</span><span class="n">end_xp</span><span class="o">-</span><span class="n">start_xp</span><span class="p">,</span><span class="s2">&quot;seconds for experience&quot;</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="s2">&quot;/&quot;</span><span class="p">,</span><span class="n">xps</span><span class="p">)</span>
      <span class="n">driver</span> <span class="o">=</span> <span class="n">parsers</span><span class="o">.</span><span class="n">JsonParser</span><span class="p">(</span><span class="s2">&quot;input_&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;/run_&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
      <span class="c1">#write latency.csv next to power_metrics.json file</span>
      <span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="s2">&quot;input_&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;/run_&quot;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">+</span><span class="s2">&quot;/latency.csv&quot;</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">latencies</span><span class="p">))</span>
</pre></div>
</div>
<p>We then run <a class="reference external" href="https://github.com/GreenAI-Uppa/AIPowerMeter/blob/main/power_metrics_management/concat_power_measure.py">concat_power_measure</a> to have the report of our experiments.</p>
<p>We fist plot the evolution of the GPU and CPU consumption of one inference for each input size. We can note a very strong linear correlation between CPU and GPU consumption, with a factor around 10 (GPU consumption is 10 times bigger than CPU). However, the regression of the consumption with respect to the size of the input is NOT linear: the consumption of one inference seems constant from 112x112 to 672x672 images, then it increases linearly until 1680x1680 images where a second jump occurs for 1792x1792 input size.</p>
<a class="reference internal image-reference" href="../_images/alexnet_nvidia_intel.png"><img alt="../_images/alexnet_nvidia_intel.png" class="align-center" src="../_images/alexnet_nvidia_intel.png" style="width: 400pt;" /></a>
<p>We also plot the behaviours of latency and total consumption as a function of the input size. We highlight a smoother evolution for the latency as the input size increases but with a still very high Pearson coefficient (0.98).</p>
<a class="reference internal image-reference" href="../_images/alexnet_gpu_latency.png"><img alt="../_images/alexnet_gpu_latency.png" class="align-center" src="../_images/alexnet_gpu_latency.png" style="width: 400pt;" /></a>
<p>As a result, for this particular experiment protocol, we can conclude that <em>the latency is a reasonable statistic to describe the energy consumption of an Alexnet at inference as a function of the input size</em>. In the next study, we will propose the same kind of analyses varying the size of the architecture (number of layers, number of filters, size of the filters).</p>
</div>
<div class="section" id="resnet-study">
<h2>Resnet study<a class="headerlink" href="#resnet-study" title="Permalink to this heading">¶</a></h2>
<p>Following the same steps as for Alexnet study, we have done the measurements with a pretrained Resnet50, using the following parameters:</p>
<ul class="simple">
<li><p>we define random colored images with an width/height of 8, 32, 64, 128, 256, 512, 1024, 2048 and 4096,</p></li>
<li><p>the number of inferences is set to 5000 for an input size of 8 up to 2048 and only 1000 for an input size of 4096.</p></li>
</ul>
<a class="reference internal image-reference" href="../_images/resnet_nvidi.png"><img alt="../_images/resnet_nvidi.png" class="align-center" src="../_images/resnet_nvidi.png" style="width: 400pt;" /></a>
<a class="reference internal image-reference" href="../_images/resnet_int_mem.png"><img alt="../_images/resnet_int_mem.png" class="align-center" src="../_images/resnet_int_mem.png" style="width: 400pt;" /></a>
<a class="reference internal image-reference" href="../_images/resnet_laten_sm.png"><img alt="../_images/resnet_laten_sm.png" class="align-center" src="../_images/resnet_laten_sm.png" style="width: 400pt;" /></a>
<a class="reference internal image-reference" href="../_images/resnet_sm_nvi.png"><img alt="../_images/resnet_sm_nvi.png" class="align-center" src="../_images/resnet_sm_nvi.png" style="width: 300pt;" /></a>
<p>We also compute the matrix of Spearman correlation :</p>
<a class="reference internal image-reference" href="../_images/resnet_spearman.png"><img alt="../_images/resnet_spearman.png" src="../_images/resnet_spearman.png" style="width: 400pt;" /></a>
</div>
<div class="section" id="bert-transformer">
<h2>Bert Transformer<a class="headerlink" href="#bert-transformer" title="Permalink to this heading">¶</a></h2>
<p>As a similar study than the previous Alexnet and Resnet one, we measure consumption of a famous NLP transformer: <a class="reference external" href="https://arxiv.org/abs/1810.04805">Bert</a>.</p>
<p>In this case we use torch with a sequence classifier version of Bert, as provided by the <a class="reference external" href="https://huggingface.co/transformers/model_doc/bert.html">hugging face</a> library.</p>
<p>The process follows previous experiments. The inputs are sequences of <strong>“yes”</strong> tokens of different sizes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># tokenizer call</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="c1"># model creation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>

<span class="c1"># use gpu to apply model</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Transform data</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;yes &quot;</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence</span><span class="o">*</span><span class="n">n_input</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span> <span class="c1"># Tokenization + format input</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<p>We chose to vary the input size from 50 to 500 tokens with a 50 token step. Each consumption measure lasts at least 20 seconds in order to have 10 measurements (we set the period parameter to 2).</p>
<p>In the following figure, we can see the evolution of GPU and CPU consumptions in Joules compared to the input size. Interestingly, the GPU consumption shows a linear correlation with the input size. On the other hand, the CPU consumption decreases until a 300 token input size then raises up to 0.26J per iteration.</p>
<a class="reference internal image-reference" href="../_images/GPU_CPU.png"><img alt="../_images/GPU_CPU.png" class="align-center" src="../_images/GPU_CPU.png" style="width: 400pt;" /></a>
<p>On the next figure, we can see the evolution of latency in seconds compared to the input size. The measure is clearly not linear as a gap appears at a size of 300-350 tokens. On this plot, the link between the latency and the energy consumption is not trivial.</p>
<a class="reference internal image-reference" href="../_images/latency.png"><img alt="../_images/latency.png" class="align-center" src="../_images/latency.png" style="width: 400pt;" /></a>
</div>
<div class="section" id="deep-rewiring">
<h2>Deep rewiring<a class="headerlink" href="#deep-rewiring" title="Permalink to this heading">¶</a></h2>
<p>In the article <a class="reference external" href="https://arxiv.org/abs/1711.05136">Training very sparse network with Deep Rewiring</a> , <a class="reference external" href="http://guillaume.bellec.eu/">G. Bellec</a> et al. introduce two algorithms allowing to train models with very low connectivy (less than 2%).
The latter are called Deep-R and Soft Deep-R.
The first one induced strong constraints on the network : we have a limited number of connectivity for each iteration of the training.
The second is a relaxed form of Deep-R where the maximal number of connections is not fixed.</p>
<p>For more details about tests and theorical guarantees on the algorithms, we invite you to read the article.
The implementations are available <a class="reference external" href="https://github.com/guillaumeBellec/deep_rewiring">here</a>.</p>
<p>At Green AI UPPA, we have measured the consumptions of three available scripts on the problem of classification for MNIST’ images.
We worked only on the CPU here.
We used the default parameters (for example 10 epochs and a batch size of 10).</p>
<ul class="simple">
<li><p>script_mnist_deep_rewiring.py is the basic implementation of Deep R,</p></li>
<li><p>script_mnist_deeep_rewiring_with_global_constraint.py,</p></li>
<li><p>script_mnist_soft_deep_rewiring.py is the implementation of soft Deep-R.</p></li>
</ul>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 23%" />
<col style="width: 27%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>CPU consumption (Joules)</p></th>
<th class="head"><p>Wattmeters measures (Joules)</p></th>
<th class="head"><p>Training duration(seconds)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Deep R</p></td>
<td><p>19490</p></td>
<td><p>28554</p></td>
<td><p>249</p></td>
</tr>
<tr class="row-odd"><td><p>Deep R Global constraints</p></td>
<td><p>19121</p></td>
<td><p>28105</p></td>
<td><p>240</p></td>
</tr>
<tr class="row-even"><td><p>Soft Deep R</p></td>
<td><p>10405</p></td>
<td><p>15655</p></td>
<td><p>130</p></td>
</tr>
</tbody>
</table>
<a class="reference internal image-reference" href="../_images/conso_dr.png"><img alt="../_images/conso_dr.png" class="align-center" src="../_images/conso_dr.png" style="width: 600pt;" /></a>
</div>
<div class="section" id="pruning">
<h2>Pruning<a class="headerlink" href="#pruning" title="Permalink to this heading">¶</a></h2>
<p>A solution for improving the size and the computation time is called pruning. It consists in selecting some neurons and removing others before, during or after the training of a model.
In this documentation, we decided to implement two solutions called SNIP (by <a class="reference external" href="https://arxiv.org/absS/1810.02340">Namhoon Lee &amp; al.</a>) and Force (by <a class="reference external" href="https://arxiv.org/absS/2006.09081">Pau de Jorge &amp; al.</a>) who both prune a model once at initialization and achieve a deletion of 95 to 99.5% parameters without losing much precision.</p>
</div>
<div class="section" id="snip">
<h2>SNIP<a class="headerlink" href="#snip" title="Permalink to this heading">¶</a></h2>
<p>For our experiments, we used an <a class="reference external" href="https://github.com/mil-ad/snip">unofficial implementation</a> using <a class="reference external" href="https://pytorch.org/">PyTorch</a> because the code proposed by the author was implemented in an old tensorflow version, which doesn’t allow the use of a GPU. We decided to test two architectures and each one with a specific dataset : respectively LeNet5-Caffe with MNIST dataset and vgg-D with CIFAR-10.
To make sure our experiment is not influenced by the random initialization of parameters, we run three times the whole training process with a different seed. At the end, we plot the precision, the computation time and of course the total energy consumption of the GPU and the machine thanks to the use of AIPowerMeter and a wattmeter.</p>
<table class="colwidths-given docutils align-default" id="id2">
<caption><span class="caption-text">Results of our experiments with SNIP</span><a class="headerlink" href="#id2" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 18%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Architecture</p></th>
<th class="head"><p>Dataset</p></th>
<th class="head"><p>Pruning ?</p></th>
<th class="head"><p>Parameters</p></th>
<th class="head"><p>Time (hh:mm:ss)</p></th>
<th class="head"><p>Max precision (%)</p></th>
<th class="head"><p>Total consumption (Wh)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>LeNet5-Caffe</p></td>
<td><p>MNIST</p></td>
<td><p>no</p></td>
<td><p>430,500</p></td>
<td><p>30:18</p></td>
<td><p>99.42</p></td>
<td><p>145.5</p></td>
</tr>
<tr class="row-odd"><td></td>
<td></td>
<td><p>yes (98%)</p></td>
<td><p>8,610</p></td>
<td><p>28:26</p></td>
<td><p>99.15</p></td>
<td><p>145.28</p></td>
</tr>
<tr class="row-even"><td><p>vgg-D</p></td>
<td><p>CIFAR-10</p></td>
<td><p>no</p></td>
<td><p>15,239,872</p></td>
<td><p>1:40:18</p></td>
<td><p>93.55</p></td>
<td><p>785</p></td>
</tr>
<tr class="row-odd"><td></td>
<td></td>
<td><p>yes (95%)</p></td>
<td><p>761,994</p></td>
<td><p>1:39:01</p></td>
<td><p>93.13</p></td>
<td><p>771</p></td>
</tr>
</tbody>
</table>
<p>We can observe the same results in precision as the paper between the original architecture and the pruned one, but our experiment cannot conclude on a significative improvement in computation time nor an economy in energy.
To understand those results, we argue that the implementation only put the value of every pruned neuron at zero, and then having a high sparsity.</p>
</div>
<div class="section" id="force">
<h2>Force<a class="headerlink" href="#force" title="Permalink to this heading">¶</a></h2>
<p>Force can be seen as an iterative SNIP as described by the authors of the algorithm. In this section,
the results of the model in term of energy consumption is described. AIPowerMeter can be used like in the previous examples
to infer the energy consumption. In this experiment, a wattmeter was also used.</p>
<p>Force was launched on CIFAR-10 with a VGG implementation. Like in the paper, the accuracy remains very high with high pruning levels.
Pruning is then a great solution to reduce the size of a neural network. However, the same conclusions appear. In term of energy
consumption, there is no clear differences between the pruned model and the full model. We infer that the reason for that is
that the optimisation algorithm is not implemented to deal with sparse matrixes. Thus, we do not obtain any gain in time complexity
and in energy consumption.</p>
<table class="colwidths-given docutils align-default" id="id3">
<caption><span class="caption-text">Results of our experiments with Force</span><a class="headerlink" href="#id3" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Pruning level</p></th>
<th class="head"><p>Accuracy</p></th>
<th class="head"><p>Inference time</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0.93</p></td>
<td><p>8.33e-3</p></td>
</tr>
<tr class="row-odd"><td><p>0.9</p></td>
<td><p>0.92</p></td>
<td><p>8.37e-3</p></td>
</tr>
</tbody>
</table>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">AI Power Meter</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../usage/quick_start.html">Quick start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../background/background.html">Background on power measure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage/in_details.html">Advanced use</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage/modules.html">Dev Documentation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Deep learning benchmark</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#experimental-protocol">Experimental protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="#alexnet-study">Alexnet study</a></li>
<li class="toctree-l2"><a class="reference internal" href="#resnet-study">Resnet study</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bert-transformer">Bert Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#deep-rewiring">Deep rewiring</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pruning">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#snip">SNIP</a></li>
<li class="toctree-l2"><a class="reference internal" href="#force">Force</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="schneiderbox.html">Machine and Deep Learning Benchmarks with wattmeters</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Bibliography</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="../usage/modules.html" title="previous chapter">Dev Documentation</a></li>
      <li>Next: <a href="schneiderbox.html" title="next chapter">Machine and Deep Learning Benchmarks with wattmeters</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, GreenAIUppa.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.3.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/experiments/experiments.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>